import os
import pandas as pd
from datetime import datetime
from utilities.inventory_files_config import base_folder, max_sheet_rows, report_api_headers, \
                                             remediation_deadline_age_days, cisa_kev_file_path
from utilities.sharepoint_api import SharePointAPI
from utilities.logger_master import logger, log_function_entry_exit
from utilities.ReportDownloader import ReportDownloader
from utilities import BasicProcessingSteps
from utilities import CommonFunctions
from utilities import StandardReportProcessingSteps
from utilities import SharePointFunctions


@log_function_entry_exit(logger)
class VulnerabilityReportProcessor:
    def __init__(self, data: pd.DataFrame, cisa_kev_df: pd.DataFrame = None, cisa_kev_file_path: str = None,
                 severity: int = 7):
        
        self.report_downloader = ReportDownloader(report_api_headers=report_api_headers)
        self.std_report_processing_steps = StandardReportProcessingSteps
        self.basic_processing_steps = BasicProcessingSteps
        self.sharepoint_functions = SharePointFunctions
        self.common_functions = CommonFunctions

        self.data = data
        self.severity = severity
        self.quit_execution = False

        self.cisa_kev_df = cisa_kev_df if cisa_kev_df else self.load_cisa_kev_file(cisa_kev_file_path)
        self.today_date_str = datetime.now().strftime("-%Y-%m-%d")
        self.unknown_regions = [['OS'], ['Network'], ['Applications']]
        self.count = []  # Initialize an empty list to store count data
        self.all_workstations = pd.DataFrame()

    def load_cisa_kev_file(self, cisa_kev_file_path):
        if cisa_kev_file_path and os.path.exists(cisa_kev_file_path):
            logger.info(f'Load CISA KEV file from path: {cisa_kev_file_path}')
            return pd.read_csv(cisa_kev_file_path)
        else:
            logger.info(f'Creating dummy CISA KEV dataframe, cisa_kev_file_path: {cisa_kev_file_path}')
            return pd.DataFrame()

    def load_report_data(self, target_filename):
        if self.quit_execution or self.skip_data_process:
            return
        self.data = self.common_functions.load_report_data(target_filename)

    # Takes a dataframe and performs all the typical process steps on it.
    def perform_standard_processing(self):
        # Every single file is filtered for the last 30 days
        self.data = self.basic_processing_steps.exclude_false_positive(self.data)
        self.data = self.basic_processing_steps.filter_to_last_30_days(self.data)
        self.data = self.basic_processing_steps.merge_severity_scores()
        # New addition: Add a column for severity level (critical, high, etc).
        # Every single filedis filtered to have only CVSSv3 Severity 7 or Higher.
        self.data = self.basic_processing_steps.filter_to_severity_7(self.data)
        self.data = self.basic_processing_steps.update_is_cisa_kev(data=self.data, cisa_kev_df=self.cisa_kev_df)
        # self.data = self.basic_processing_steps.update_remediation_deadline(data=self.data, 
        #                          remediation_deadline_age_days=remediation_deadline_age_days) # Commented since dates are corrupted in db
        self.data = self.basic_processing_steps.add_severity_column(self.data)
        self.data = self.basic_processing_steps.add_unique_id_column(self.data)
        self.data.fillna('', inplace=True)  # Remove any empty entries

    def check_data_status(self, tag=''):
        if self.quit_execution:
            return 'QUIT'
        elif self.skip_data_process:
            return 'SKIP'
        elif len(self.data) == 0 and self.merge_files_dict and self.merge_data_folder:
            logger.error(f"Data is empty for file: {self.filename} : {tag}")
            logger.error(f"quit execution!")
            self.quit_execution = True
            return 'QUIT'
        elif len(self.data) == 0:

            logger.error(f"Data is empty for file: {self.filename} : {tag}")
            self.quit_execution = False
            return 'SKIP'
        else:
            self.quit_execution = False
            return 'CONTINUE'

    def download_reports(self):
        for self.report_id, self.filename in self.report_dict.items():
            try:
                self.quit_execution = False
                self.skip_data_process = False
                if not self.filename or (self.filename and len(self.filename) == ''):
                    logger.error(f"Unable to determine filename for given report ID: {self.report_id}")
                    continue

                # Download the equivalent report to a dataframe
                target_filename = os.path.join(self.raw_data_path, self.filename + ".csv")
                if not self.common_functions.check_if_file_downloaded_recently(target_filename):
                    logger.info(f"Generating dataframe for {self.filename}...")
                    self.report_downloader.download_report_from_api(self.report_id, target_filename)
                    logger.info(f"...done generating.")
                else:
                    logger.info(f"file already downloaded proceed with processing {self.filename}...")

            except Exception as e:
                logger.error(f"Error processing report {self.filename}: {str(e)}")
                continue

    def process_reports(self):
        for file_name in os.listdir(self.raw_data_path):
            logger.info(f'process report start - {file_name}')
            try:
                self.quit_execution = False
                self.skip_data_process = False
                target_filename = os.path.join(self.raw_data_path, file_name)
                target_filename_asset_count = os.path.join(self.raw_data_path, file_name.split('.')[0] + '_assets_count.csv')
                if not self.common_functions.check_if_file_is_valid(target_filename):
                    logger.error(f"...EMPTY FILE SKIPPED. filename {self.filename}")
                    continue
                logger.info(f'process report target_filename - {target_filename}')
                self.filename = file_name.split('.')[0]
                self.load_report_data(target_filename)
                logger.info(f'process report check status - {target_filename}')
                # check if data processing steps needs to be skipped
                data_status = self.check_data_status()
                # QUIT: if data is empty and merge files is needed
                if data_status == 'QUIT':
                    return
                # SKIP: if data is empty and merge files is not needed
                elif data_status == 'SKIP':
                    continue
                logger.info(
                    f'process report perform_standard_processing - filename: {self.filename}, target: {target_filename}')

                # Execute standard processing steps (merge severity scores, assign severity labels, etc.)
                self.perform_standard_processing()
                logger.info(
                    f'process report update_cisa_kev_column_position - filename: {self.filename}, target: {target_filename}')
                self.data = self.basic_processing_steps.update_cisa_kev_column_position(data=self.data)
                print('create target_filename_asset_count', target_filename_asset_count)
                tmp_data = self.data.copy()
                tmp_data = tmp_data.groupby('Asset Names').size().reset_index(name='asset_count').to_csv(
                    target_filename_asset_count, index=False)
                print('create target_filename_asset_count Completed')

                if self.report_name == 'Standard':
                    logger.info(
                        f"Start standard report processing... - filename: {self.filename}, target: {target_filename}")
                    self.std_report_processing_steps.run_standard_report_process_steps()

                else:
                    # continue with processing.
                    logger.info(
                        f"Splitting original dataframe into sheets if needed... - filename: {self.filename}, target: {target_filename}")
                    if len(self.data) > 0:
                        sheets_list = self.common_functions.split_dataframe(data=self.data, max_sheet_rows=max_sheet_rows)
                        excel_file_path = os.path.join(self.processed_data_path, self.filename + self.today_date_str + ".xlsx")
                        self.publish_data_into_excel_file_with_sheets(excel_file_path, sheets_list)
                    else:
                        logger.error(f"...EMPTY DF SKIPPED. filename {self.filename}")
            except Exception as e:
                logger.error(f"Error processing report {self.filename}: {str(e)}")
                continue
                
    def upload_to_sharepoint(self):
        if self.quit_execution:
            logger.debug("======= SKIPPING PUSH TO SHAREPOINT - quit_execution is triggered")
            return None
        
        self.sharepoint_functions.upload_to_sharepoint(latest_folder_path=self.latest_folder_path, 
                                                        latest_folder_name=self.latest_folder_name, 
                                                        history_folder_path=self.history_folder_path, 
                                                        history_folder_name=self.history_folder_name, 
                                                        processed_data_path=self.processed_data_path, 
                                                        merge_data_folder=self.merge_data_folder,
                                                        merge_data_path=self.merge_data_path, 
                                                        merge_files_dict=self.merge_files_dict)
        
    def manage_reports(self):

        logger.info(f"START OF EXECUTION FOR REPORT - {self.report_name}")
        if self.download_new_reports:
            logger.info("Downloading new reports...")
            self.download_reports()
            logger.info("processing new reports...")
            self.process_reports()
            if self.report_name == 'Standard':
                self.std_report_processing_steps.process_all_workstations_unknownregions(
                                                                all_workstations=self.all_workstations, 
                                                                unknown_regions=self.unknown_regions, 
                                                                count=self.count, 
                                                                processed_data_path=self.processed_data_path, 
                                                                today_date_str=self.today_date_str)
                
            self.basic_processing_steps.merge_split_files_to_master_excel_file(self.merge_files_dict, self.merge_data_path, 
                                                                                self.processed_data_path, self.today_date_str, 
                                                                                self.merge_files_sheets)
        else:
            logger.info("Bypassing download of new reports. Continuing the publication step.")
        # self.upload_to_sharepoint()
        logger.info(f"END OF EXECUTION FOR REPORT - {self.report_name}")
        # TODO: Delete the local files once finished?

    def run(self):
        self.manage_reports()
